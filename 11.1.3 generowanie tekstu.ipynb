{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60951f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 22:31:53.228350: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-14 22:31:53.269649: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-14 22:31:53.815226: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c637c3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 22:31:54.635693: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-01-14 22:31:54.635755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: dziobak\n",
      "2024-01-14 22:31:54.635769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: dziobak\n",
      "2024-01-14 22:31:54.635904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 520.61.5\n",
      "2024-01-14 22:31:54.635949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 520.61.5\n",
      "2024-01-14 22:31:54.635961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 520.61.5\n",
      "2024-01-14 22:31:54.858343: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-01-14 22:31:54.859341: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-01-14 22:31:54.860246: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 371, 50)           600450    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               60400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 12009)             1212909   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,873,759\n",
      "Trainable params: 1,873,759\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Wczytanie modelu\n",
    "model = tf.keras.models.load_model(\"moj_model.h5\")\n",
    "\n",
    "# Wyświetlenie struktury modelu\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c0f1f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Na śniadanie zjadłem ciasto być na kraju pan tadzio mówił do siebie — wujaszku mógł przyjść — ale za ktoś zresztą widzę ze starości wilgotnieją mi oczy… połowę majątku powiadasz nieszczęśliwym dzieciom nieźle nieźle powiedz jej… po co zresztą ty masz mówić skoro ja jej sama powiem… prędko prędko czemu prędko prędko czemu mi wujaszek bracie pewnie biedna biedna pani biedna basieńka pani do tego tylko można by ją samą wył z radości… ten klasa zatrzymać mnie sama taka umiejąc sobie dać to po dziesięć lat nie widzi już po schodach zlituję stał po katastrofie z nich słuchać boże zamknął śmierć — jak ją\n"
     ]
    }
   ],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_length):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
    "        \n",
    "        # Wybieranie indeksu z najwyższym prawdopodobieństwem\n",
    "        predicted_index = tf.argmax(predicted_probs, axis=-1).numpy()\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "# Generowanie tekstu z seed_text o długości 10 słów\n",
    "generated_text = generate_text(\"Na śniadanie zjadłem ciasto\", 100, model, 372)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ef745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
